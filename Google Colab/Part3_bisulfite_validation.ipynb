{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOgrNI6q3BYoSasy0KXhe6h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZJaBDFDhhJyp","executionInfo":{"status":"ok","timestamp":1753294945238,"user_tz":420,"elapsed":5404,"user":{"displayName":"Ashok Kumar Sharma","userId":"18215034872609959171"}},"outputId":"13b1e33c-2385-40a2-c4b2-b7038e2c6df0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Bisulfite Conversion Efficiency Validation and Visualization Module\n","This module provides comprehensive validation and visualization tools.\n","Use in conjunction with simulation and analysis modules for complete pipeline.\n"]}],"source":["#!/usr/bin/env python3\n","\"\"\"\n","Bisulfite Conversion Efficiency Analysis Pipeline\n","Part 3: Validation and Visualization\n","\n","This module provides comprehensive validation methods and visualization tools\n","for assessing the accuracy of bisulfite conversion efficiency measurements.\n","\"\"\"\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy import stats\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from collections import defaultdict\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Set style for publication-quality plots\n","plt.style.use('seaborn-v0_8')\n","sns.set_palette(\"husl\")\n","\n","class ValidationFramework:\n","    \"\"\"\n","    Comprehensive validation framework for bisulfite conversion efficiency analysis\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.validation_results = {}\n","        self.benchmark_data = {}\n","\n","    def create_ground_truth_benchmarks(self, efficiency_range=(0.90, 0.999), n_points=10):\n","        \"\"\"\n","        Create ground truth benchmarks with known conversion efficiencies\n","\n","        Args:\n","            efficiency_range (tuple): Min and max conversion efficiencies\n","            n_points (int): Number of benchmark points to create\n","\n","        Returns:\n","            list: List of benchmark conversion efficiencies\n","        \"\"\"\n","        print(\"Creating ground truth benchmarks...\")\n","\n","        min_eff, max_eff = efficiency_range\n","\n","        # Create both linear and non-linear spacing for comprehensive testing\n","        linear_points = np.linspace(min_eff, max_eff, n_points // 2)\n","\n","        # Add some challenging points near high efficiency\n","        challenging_points = np.array([0.95, 0.98, 0.985, 0.99, 0.995, 0.999])\n","        challenging_points = challenging_points[challenging_points <= max_eff]\n","\n","        # Combine and sort\n","        all_points = np.unique(np.concatenate([linear_points, challenging_points]))\n","        all_points = all_points[:n_points]  # Limit to requested number\n","\n","        print(f\"Created {len(all_points)} benchmark points: {all_points}\")\n","        return all_points.tolist()\n","\n","    def validate_method_accuracy(self, analysis_results, true_efficiencies):\n","        \"\"\"\n","        Validate the accuracy of conversion efficiency measurement methods\n","\n","        Args:\n","            analysis_results (dict): Results from efficiency analysis\n","            true_efficiencies (list): Known true conversion efficiencies\n","\n","        Returns:\n","            dict: Comprehensive validation metrics\n","        \"\"\"\n","        print(\"Validating method accuracy...\")\n","\n","        validation_metrics = {}\n","\n","        # Extract estimated efficiencies from different methods\n","        methods = ['non_cpg', 'lambda_dna', 'chh_context', 'confidence_interval']\n","\n","        for method in methods:\n","            estimated_effs = []\n","            true_effs = []\n","\n","            for true_eff, results in zip(true_efficiencies, analysis_results.values()):\n","                if method == 'confidence_interval':\n","                    est_eff = results.get('confidence_intervals', {}).get('mean_efficiency')\n","                else:\n","                    method_key = f'{method}_efficiency' if method != 'lambda_dna' else 'lambda_efficiency'\n","                    est_eff = results.get(method_key, {}).get('efficiency')\n","\n","                if est_eff is not None:\n","                    estimated_effs.append(est_eff)\n","                    true_effs.append(true_eff)\n","\n","            if len(estimated_effs) >= 3:  # Minimum points for meaningful validation\n","                method_validation = self._calculate_validation_metrics(true_effs, estimated_effs)\n","                method_validation['method_name'] = method\n","                method_validation['n_points'] = len(estimated_effs)\n","                validation_metrics[method] = method_validation\n","\n","        # Overall consensus validation\n","        consensus_effs = []\n","        consensus_true = []\n","\n","        for true_eff, results in zip(true_efficiencies, analysis_results.values()):\n","            summary = results.get('summary_metrics', {})\n","            consensus_eff = summary.get('consensus_efficiency')\n","\n","            if consensus_eff is not None:\n","                consensus_effs.append(consensus_eff)\n","                consensus_true.append(true_eff)\n","\n","        if len(consensus_effs) >= 3:\n","            consensus_validation = self._calculate_validation_metrics(consensus_true, consensus_effs)\n","            consensus_validation['method_name'] = 'consensus'\n","            consensus_validation['n_points'] = len(consensus_effs)\n","            validation_metrics['consensus'] = consensus_validation\n","\n","        # Cross-method consistency analysis\n","        consistency_analysis = self._analyze_method_consistency(analysis_results)\n","        validation_metrics['consistency_analysis'] = consistency_analysis\n","\n","        return validation_metrics\n","\n","    def _calculate_validation_metrics(self, true_values, predicted_values):\n","        \"\"\"Calculate comprehensive validation metrics\"\"\"\n","\n","        true_arr = np.array(true_values)\n","        pred_arr = np.array(predicted_values)\n","\n","        # Basic error metrics\n","        mae = mean_absolute_error(true_arr, pred_arr)\n","        mse = mean_squared_error(true_arr, pred_arr)\n","        rmse = np.sqrt(mse)\n","\n","        # Relative metrics\n","        mape = np.mean(np.abs((true_arr - pred_arr) / true_arr)) * 100  # Mean Absolute Percentage Error\n","\n","        # Correlation metrics\n","        r2 = r2_score(true_arr, pred_arr)\n","        pearson_r, pearson_p = stats.pearsonr(true_arr, pred_arr)\n","        spearman_r, spearman_p = stats.spearmanr(true_arr, pred_arr)\n","\n","        # Bias metrics\n","        bias = np.mean(pred_arr - true_arr)\n","        relative_bias = bias / np.mean(true_arr) * 100\n","\n","        # Residual analysis\n","        residuals = pred_arr - true_arr\n","        residual_std = np.std(residuals)\n","\n","        # Confidence metrics (within X% accuracy)\n","        within_1pct = np.mean(np.abs(residuals) <= 0.01) * 100\n","        within_2pct = np.mean(np.abs(residuals) <= 0.02) * 100\n","        within_5pct = np.mean(np.abs(residuals) <= 0.05) * 100\n","\n","        return {\n","            'mae': mae,\n","            'mse': mse,\n","            'rmse': rmse,\n","            'mape': mape,\n","            'r2_score': r2,\n","            'pearson_correlation': pearson_r,\n","            'pearson_p_value': pearson_p,\n","            'spearman_correlation': spearman_r,\n","            'spearman_p_value': spearman_p,\n","            'bias': bias,\n","            'relative_bias_percent': relative_bias,\n","            'residual_std': residual_std,\n","            'within_1pct_accuracy': within_1pct,\n","            'within_2pct_accuracy': within_2pct,\n","            'within_5pct_accuracy': within_5pct,\n","            'true_values': true_values,\n","            'predicted_values': predicted_values.tolist(),\n","            'residuals': residuals.tolist()\n","        }\n","\n","    def _analyze_method_consistency(self, analysis_results):\n","        \"\"\"Analyze consistency between different measurement methods\"\"\"\n","\n","        # Extract all method estimates for each sample\n","        method_estimates = defaultdict(list)\n","        sample_ids = list(analysis_results.keys())\n","\n","        for sample_id, results in analysis_results.items():\n","            # Extract estimates from each method\n","            non_cpg = results.get('non_cpg_efficiency', {}).get('efficiency')\n","            lambda_dna = results.get('lambda_efficiency', {}).get('efficiency')\n","            chh = results.get('chh_efficiency', {}).get('efficiency')\n","            ci = results.get('confidence_intervals', {}).get('mean_efficiency')\n","\n","            if non_cpg is not None:\n","                method_estimates['non_cpg'].append(non_cpg)\n","            if lambda_dna is not None:\n","                method_estimates['lambda_dna'].append(lambda_dna)\n","            if chh is not None:\n","                method_estimates['chh'].append(chh)\n","            if ci is not None:\n","                method_estimates['confidence_interval'].append(ci)\n","\n","        # Calculate pairwise correlations between methods\n","        method_correlations = {}\n","        methods = list(method_estimates.keys())\n","\n","        for i, method1 in enumerate(methods):\n","            for j, method2 in enumerate(methods[i+1:], i+1):\n","                if (len(method_estimates[method1]) >= 3 and\n","                    len(method_estimates[method2]) >= 3):\n","\n","                    # Match up samples that have both estimates\n","                    paired_est1 = []\n","                    paired_est2 = []\n","\n","                    min_len = min(len(method_estimates[method1]), len(method_estimates[method2]))\n","                    paired_est1 = method_estimates[method1][:min_len]\n","                    paired_est2 = method_estimates[method2][:min_len]\n","\n","                    if len(paired_est1) >= 3:\n","                        corr, p_val = stats.pearsonr(paired_est1, paired_est2)\n","                        method_correlations[f'{method1}_vs_{method2}'] = {\n","                            'correlation': corr,\n","                            'p_value': p_val,\n","                            'n_samples': len(paired_est1)\n","                        }\n","\n","        # Calculate coefficient of variation across methods for each sample\n","        sample_cvs = []\n","        for sample_id, results in analysis_results.items():\n","            estimates = []\n","            summary = results.get('summary_metrics', {})\n","            method_effs = summary.get('methods_efficiencies', {})\n","\n","            estimates = list(method_effs.values())\n","\n","            if len(estimates) >= 2:\n","                cv = np.std(estimates) / np.mean(estimates) if np.mean(estimates) > 0 else 0\n","                sample_cvs.append(cv)\n","\n","        mean_cv = np.mean(sample_cvs) if sample_cvs else 0\n","\n","        return {\n","            'method_correlations': method_correlations,\n","            'mean_coefficient_variation': mean_cv,\n","            'cv_distribution': sample_cvs,\n","            'n_samples_analyzed': len(sample_cvs)\n","        }\n","\n","class VisualizationSuite:\n","    \"\"\"\n","    Comprehensive visualization suite for bisulfite conversion efficiency analysis\n","    \"\"\"\n","\n","    def __init__(self, figsize=(12, 8), dpi=300):\n","        self.figsize = figsize\n","        self.dpi = dpi\n","\n","    def create_accuracy_plots(self, validation_results, save_path=None):\n","        \"\"\"\n","        Create comprehensive accuracy validation plots\n","\n","        Args:\n","            validation_results (dict): Results from validation framework\n","            save_path (str): Optional path to save plots\n","\n","        Returns:\n","            dict: Figure objects for further customization\n","        \"\"\"\n","        print(\"Creating accuracy validation plots...\")\n","\n","        figures = {}\n","\n","        # 1. Method comparison scatter plots\n","        fig1 = self._plot_method_accuracy_comparison(validation_results)\n","        figures['method_comparison'] = fig1\n","\n","        # 2. Error distribution plots\n","        fig2 = self._plot_error_distributions(validation_results)\n","        figures['error_distributions'] = fig2\n","\n","        # 3. Residual analysis plots\n","        fig3 = self._plot_residual_analysis(validation_results)\n","        figures['residual_analysis'] = fig3\n","\n","        # 4. Method consistency plots\n","        fig4 = self._plot_method_consistency(validation_results)\n","        figures['method_consistency'] = fig4\n","\n","        if save_path:\n","            for name, fig in figures.items():\n","                fig.savefig(f\"{save_path}_{name}.png\", dpi=self.dpi, bbox_inches='tight')\n","                print(f\"Saved {name} plot to {save_path}_{name}.png\")\n","\n","        return figures\n","\n","    def _plot_method_accuracy_comparison(self, validation_results):\n","        \"\"\"Create scatter plots comparing predicted vs true values for each method\"\"\"\n","\n","        methods_to_plot = [k for k in validation_results.keys()\n","                          if k != 'consistency_analysis' and 'true_values' in validation_results[k]]\n","\n","        n_methods = len(methods_to_plot)\n","        if n_methods == 0:\n","            print(\"No methods with sufficient data for plotting\")\n","            return None\n","\n","        # Create subplots\n","        cols = min(3, n_methods)\n","        rows = (n_methods + cols - 1) // cols\n","\n","        fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows), dpi=self.dpi)\n","        if n_methods == 1:\n","            axes = [axes]\n","        elif rows == 1:\n","            axes = [axes] if n_methods == 1 else axes\n","        else:\n","            axes = axes.flatten()\n","\n","        for i, method in enumerate(methods_to_plot):\n","            ax = axes[i] if n_methods > 1 else axes[0]\n","\n","            data = validation_results[method]\n","            true_vals = np.array(data['true_values'])\n","            pred_vals = np.array(data['predicted_values'])\n","\n","            # Scatter plot\n","            ax.scatter(true_vals, pred_vals, alpha=0.7, s=50)\n","\n","            # Perfect prediction line\n","            min_val = min(np.min(true_vals), np.min(pred_vals))\n","            max_val = max(np.max(true_vals), np.max(pred_vals))\n","            ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, label='Perfect prediction')\n","\n","            # Fit line\n","            z = np.polyfit(true_vals, pred_vals, 1)\n","            p = np.poly1d(z)\n","            ax.plot(true_vals, p(true_vals), 'g-', alpha=0.8, label=f'Fit: y={z[0]:.3f}x+{z[1]:.3f}')\n","\n","            # Formatting\n","            ax.set_xlabel('True Conversion Efficiency')\n","            ax.set_ylabel('Predicted Conversion Efficiency')\n","            ax.set_title(f'{method.replace(\"_\", \" \").title()} Method\\n'\n","                        f'R² = {data[\"r2_score\"]:.3f}, RMSE = {data[\"rmse\"]:.4f}')\n","            ax.legend()\n","            ax.grid(True, alpha=0.3)\n","\n","            # Set equal aspect ratio\n","            ax.set_aspect('equal', adjustable='box')\n","\n","        # Hide unused subplots\n","        for i in range(n_methods, len(axes)):\n","            axes[i].set_visible(False)\n","\n","        plt.tight_layout()\n","        plt.suptitle('Method Accuracy Comparison: Predicted vs True Conversion Efficiency',\n","                     fontsize=16, y=1.02)\n","\n","        return fig\n","\n","    def _plot_error_distributions(self, validation_results):\n","        \"\"\"Create error distribution plots for each method\"\"\"\n","\n","        methods_to_plot = [k for k in validation_results.keys()\n","                          if k != 'consistency_analysis' and 'residuals' in validation_results[k]]\n","\n","        if not methods_to_plot:\n","            print(\"No methods with residual data for plotting\")\n","            return None\n","\n","        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=self.figsize, dpi=self.dpi)\n","\n","        # 1. Residual distributions (histogram)\n","        all_residuals = []\n","        method_names = []\n","        colors = plt.cm.Set3(np.linspace(0, 1, len(methods_to_plot)))\n","\n","        for i, method in enumerate(methods_to_plot):\n","            residuals = validation_results[method]['residuals']\n","            all_residuals.extend(residuals)\n","            method_names.extend([method.replace('_', ' ').title()] * len(residuals))\n","\n","            ax1.hist(residuals, bins=15, alpha=0.6, label=method.replace('_', ' ').title(),\n","                    color=colors[i], density=True)\n","\n","        ax1.axvline(x=0, color='red', linestyle='--', alpha=0.8, label='Perfect accuracy')\n","        ax1.set_xlabel('Residuals (Predicted - True)')\n","        ax1.set_ylabel('Density')\n","        ax1.set_title('Error Distribution by Method')\n","        ax1.legend()\n","        ax1.grid(True, alpha=0.3)\n","\n","        # 2. Box plot of absolute errors\n","        abs_errors_by_method = []\n","        method_labels = []\n","\n","        for method in methods_to_plot:\n","            residuals = np.array(validation_results[method]['residuals'])\n","            abs_errors = np.abs(residuals)\n","            abs_errors_by_method.append(abs_errors)\n","            method_labels.append(method.replace('_', ' ').title())\n","\n","        bp = ax2.boxplot(abs_errors_by_method, labels=method_labels, patch_artist=True)\n","        for patch, color in zip(bp['boxes'], colors):\n","            patch.set_facecolor(color)\n","            patch.set_alpha(0.6)\n","\n","        ax2.set_ylabel('Absolute Error')\n","        ax2.set_title('Absolute Error Distribution')\n","        ax2.grid(True, alpha=0.3)\n","        plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')\n","\n","        # 3. Accuracy within tolerance levels\n","        tolerance_levels = [0.01, 0.02, 0.05]\n","        method_accuracies = {method: [] for method in methods_to_plot}\n","\n","        for method in methods_to_plot:\n","            data = validation_results[method]\n","            accuracies = [\n","                data.get('within_1pct_accuracy', 0),\n","                data.get('within_2pct_accuracy', 0),\n","                data.get('within_5pct_accuracy', 0)\n","            ]\n","            method_accuracies[method] = accuracies\n","\n","        x = np.arange(len(tolerance_levels))\n","        width = 0.8 / len(methods_to_plot)\n","\n","        for i, method in enumerate(methods_to_plot):\n","            offset = (i - len(methods_to_plot)/2 + 0.5) * width\n","            ax3.bar(x + offset, method_accuracies[method], width,\n","                   label=method.replace('_', ' ').title(),\n","                   color=colors[i], alpha=0.7)\n","\n","        ax3.set_xlabel('Tolerance Level')\n","        ax3.set_ylabel('Percentage of Predictions (%)')\n","        ax3.set_title('Accuracy Within Tolerance Levels')\n","        ax3.set_xticks(x)\n","        ax3.set_xticklabels(['±1%', '±2%', '±5%'])\n","        ax3.legend()\n","        ax3.grid(True, alpha=0.3)\n","\n","        # 4. Performance metrics comparison\n","        metrics = ['mae', 'rmse', 'mape', 'r2_score']\n","        metric_labels = ['MAE', 'RMSE', 'MAPE (%)', 'R²']\n","\n","        # Normalize metrics for comparison (except R²)\n","        normalized_data = []\n","        for method in methods_to_plot:\n","            data = validation_results[method]\n","            method_metrics = [\n","                data.get('mae', 0),\n","                data.get('rmse', 0),\n","                data.get('mape', 0),\n","                data.get('r2_score', 0)\n","            ]\n","            normalized_data.append(method_metrics)\n","\n","        # Create radar-like comparison\n","        x_pos = np.arange(len(metric_labels))\n","        for i, method in enumerate(methods_to_plot):\n","            ax4.plot(x_pos, normalized_data[i], 'o-',\n","                    label=method.replace('_', ' ').title(),\n","                    color=colors[i], linewidth=2, markersize=6)\n","\n","        ax4.set_xlabel('Performance Metrics')\n","        ax4.set_ylabel('Metric Value')\n","        ax4.set_title('Performance Metrics Comparison')\n","        ax4.set_xticks(x_pos)\n","        ax4.set_xticklabels(metric_labels)\n","        ax4.legend()\n","        ax4.grid(True, alpha=0.3)\n","\n","        plt.tight_layout()\n","        return fig\n","\n","    def _plot_residual_analysis(self, validation_results):\n","        \"\"\"Create detailed residual analysis plots\"\"\"\n","\n","        methods_to_plot = [k for k in validation_results.keys()\n","                          if k != 'consistency_analysis' and 'residuals' in validation_results[k]]\n","\n","        if not methods_to_plot:\n","            return None\n","\n","        fig, axes = plt.subplots(2, 2, figsize=self.figsize, dpi=self.dpi)\n","        colors = plt.cm.Set3(np.linspace(0, 1, len(methods_to_plot)))\n","\n","        # 1. Residuals vs Predicted Values\n","        for i, method in enumerate(methods_to_plot):\n","            data = validation_results[method]\n","            predicted = np.array(data['predicted_values'])\n","            residuals = np.array(data['residuals'])\n","\n","            axes[0,0].scatter(predicted, residuals, alpha=0.6,\n","                            label=method.replace('_', ' ').title(),\n","                            color=colors[i], s=30)\n","\n","        axes[0,0].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n","        axes[0,0].set_xlabel('Predicted Values')\n","        axes[0,0].set_ylabel('Residuals')\n","        axes[0,0].set_title('Residuals vs Predicted Values')\n","        axes[0,0].legend()\n","        axes[0,0].grid(True, alpha=0.3)\n","\n","        # 2. Q-Q plots for normality assessment\n","        from scipy.stats import probplot\n","\n","        for i, method in enumerate(methods_to_plot):\n","            residuals = np.array(validation_results[method]['residuals'])\n","            probplot(residuals, dist=\"norm\", plot=axes[0,1])\n","\n","        axes[0,1].set_title('Q-Q Plot: Residual Normality')\n","        axes[0,1].grid(True, alpha=0.3)\n","\n","        # 3. Residuals vs True Values\n","        for i, method in enumerate(methods_to_plot):\n","            data = validation_results[method]\n","            true_vals = np.array(data['true_values'])\n","            residuals = np.array(data['residuals'])\n","\n","            axes[1,0].scatter(true_vals, residuals, alpha=0.6,\n","                            label=method.replace('_', ' ').title(),\n","                            color=colors[i], s=30)\n","\n","        axes[1,0].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n","        axes[1,0].set_xlabel('True Values')\n","        axes[1,0].set_ylabel('Residuals')\n","        axes[1,0].set_title('Residuals vs True Values')\n","        axes[1,0].legend()\n","        axes[1,0].grid(True, alpha=0.3)\n","\n","        # 4. Absolute residuals vs predicted (heteroscedasticity check)\n","        for i, method in enumerate(methods_to_plot):\n","            data = validation_results[method]\n","            predicted = np.array(data['predicted_values'])\n","            abs_residuals = np.abs(np.array(data['residuals']))\n","\n","            axes[1,1].scatter(predicted, abs_residuals, alpha=0.6,\n","                            label=method.replace('_', ' ').title(),\n","                            color=colors[i], s=30)\n","\n","        axes[1,1].set_xlabel('Predicted Values')\n","        axes[1,1].set_ylabel('|Residuals|')\n","        axes[1,1].set_title('Absolute Residuals vs Predicted')\n","        axes[1,1].legend()\n","        axes[1,1].grid(True, alpha=0.3)\n","\n","        plt.tight_layout()\n","        return fig\n","\n","    def _plot_method_consistency(self, validation_results):\n","        \"\"\"Create method consistency analysis plots\"\"\"\n","\n","        consistency_data = validation_results.get('consistency_analysis')\n","        if not consistency_data:\n","            print(\"No consistency analysis data available\")\n","            return None\n","\n","        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=self.figsize, dpi=self.dpi)\n","\n","        # 1. Method correlation heatmap\n","        correlations = consistency_data.get('method_correlations', {})\n","        if correlations:\n","            # Extract correlation matrix\n","            methods = set()\n","            for pair in correlations.keys():\n","                method1, method2 = pair.split('_vs_')\n","                methods.add(method1)\n","                methods.add(method2)\n","\n","            methods = sorted(list(methods))\n","            n_methods = len(methods)\n","\n","            if n_methods > 1:\n","                corr_matrix = np.eye(n_methods)\n","\n","                for i, method1 in enumerate(methods):\n","                    for j, method2 in enumerate(methods):\n","                        if i != j:\n","                            pair_key1 = f\"{method1}_vs_{method2}\"\n","                            pair_key2 = f\"{method2}_vs_{method1}\"\n","\n","                            if pair_key1 in correlations:\n","                                corr_matrix[i,j] = correlations[pair_key1]['correlation']\n","                            elif pair_key2 in correlations:\n","                                corr_matrix[i,j] = correlations[pair_key2]['correlation']\n","\n","                im = ax1.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n","                ax1.set_xticks(range(n_methods))\n","                ax1.set_yticks(range(n_methods))\n","                ax1.set_xticklabels([m.replace('_', ' ').title() for m in methods], rotation=45)\n","                ax1.set_yticklabels([m.replace('_', ' ').title() for m in methods])\n","\n","                # Add correlation values as text\n","                for i in range(n_methods):\n","                    for j in range(n_methods):\n","                        text = ax1.text(j, i, f'{corr_matrix[i,j]:.2f}',\n","                                       ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n","\n","                plt.colorbar(im, ax=ax1)\n","                ax1.set_title('Inter-Method Correlations')\n","\n","        # 2. Coefficient of variation distribution\n","        cv_data = consistency_data.get('cv_distribution', [])\n","        if cv_data:\n","            ax2.hist(cv_data, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n","            ax2.axvline(x=np.mean(cv_data), color='red', linestyle='--',\n","                       label=f'Mean CV = {np.mean(cv_data):.3f}')\n","            ax2.set_xlabel('Coefficient of Variation')\n","            ax2.set_ylabel('Frequency')\n","            ax2.set_title('Method Agreement Distribution')\n","            ax2.legend()\n","            ax2.grid(True, alpha=0.3)\n","\n","        # 3. Pairwise method comparisons\n","        if len(correlations) > 0:\n","            pairs = list(correlations.keys())[:6]  # Show top 6 pairs\n","            corr_values = [correlations[pair]['correlation'] for pair in pairs]\n","            n_samples = [correlations[pair]['n_samples'] for pair in pairs]\n","\n","            # Create bubble chart\n","            x_pos = range(len(pairs))\n","            colors_bubble = plt.cm.viridis(np.array(n_samples) / max(n_samples))\n","\n","            scatter = ax3.scatter(x_pos, corr_values, s=[n*20 for n in n_samples],\n","                                c=colors_bubble, alpha=0.6, edgecolors='black')\n","\n","            ax3.set_xticks(x_pos)\n","            ax3.set_xticklabels([pair.replace('_vs_', ' vs\\n').replace('_', ' ').title()\n","                               for pair in pairs], rotation=45)\n","            ax3.set_ylabel('Correlation Coefficient')\n","            ax3.set_title('Pairwise Method Correlations\\n(Bubble size = sample size)')\n","            ax3.grid(True, alpha=0.3)\n","            ax3.set_ylim(-0.1, 1.1)\n","\n","        # 4. Method reliability summary\n","        # Create a summary plot showing key metrics for each method\n","        method_names = []\n","        mae_values = []\n","        r2_values = []\n","        within_2pct = []\n","\n","        for method, data in validation_results.items():\n","            if method != 'consistency_analysis' and 'mae' in data:\n","                method_names.append(method.replace('_', ' ').title())\n","                mae_values.append(data.get('mae', 0))\n","                r2_values.append(data.get('r2_score', 0))\n","                within_2pct.append(data.get('within_2pct_accuracy', 0))\n","\n","        if method_names:\n","            x_pos = np.arange(len(method_names))\n","\n","            # Create twin axis for different scales\n","            ax4_twin = ax4.twinx()\n","\n","            # Plot MAE as bars\n","            bars1 = ax4.bar(x_pos - 0.2, mae_values, 0.4, label='MAE', alpha=0.7, color='lightcoral')\n","            # Plot R² as line\n","            line1 = ax4_twin.plot(x_pos, r2_values, 'go-', label='R²', linewidth=2, markersize=6)\n","\n","            ax4.set_xlabel('Methods')\n","            ax4.set_ylabel('Mean Absolute Error', color='red')\n","            ax4_twin.set_ylabel('R² Score', color='green')\n","            ax4.set_title('Method Performance Summary')\n","            ax4.set_xticks(x_pos)\n","            ax4.set_xticklabels(method_names, rotation=45)\n","\n","            # Add accuracy percentages as text\n","            for i, (mae, r2, acc) in enumerate(zip(mae_values, r2_values, within_2pct)):\n","                ax4.text(i, mae + max(mae_values)*0.05, f'{acc:.0f}%',\n","                        ha='center', va='bottom', fontsize=8, fontweight='bold')\n","\n","            ax4.grid(True, alpha=0.3)\n","\n","        plt.tight_layout()\n","        return fig\n","\n","    def create_comprehensive_report(self, validation_results):\n","        \"\"\"\n","        Create a comprehensive text report of validation results\n","\n","        Args:\n","            validation_results (dict): Results from validation framework\n","\n","        Returns:\n","            str: Formatted validation report\n","        \"\"\"\n","        print(\"Generating comprehensive validation report...\")\n","\n","        report = []\n","        report.append(\"=\" * 80)\n","        report.append(\"BISULFITE CONVERSION EFFICIENCY VALIDATION REPORT\")\n","        report.append(\"=\" * 80)\n","        report.append(\"\")\n","\n","        # Overall summary\n","        methods_analyzed = [k for k in validation_results.keys() if k != 'consistency_analysis']\n","        report.append(f\"Total methods analyzed: {len(methods_analyzed)}\")\n","        report.append(\"\")\n","\n","        # Method-specific results\n","        for method, data in validation_results.items():\n","            if method == 'consistency_analysis':\n","                continue\n","\n","            report.append(f\"METHOD: {method.replace('_', ' ').upper()}\")\n","            report.append(\"-\" * 40)\n","\n","            if 'n_points' in data:\n","                report.append(f\"Sample size: {data['n_points']} data points\")\n","\n","            # Accuracy metrics\n","            report.append(\"Accuracy Metrics:\")\n","            if 'mae' in data:\n","                report.append(f\"  • Mean Absolute Error (MAE): {data['mae']:.6f}\")\n","            if 'rmse' in data:\n","                report.append(f\"  • Root Mean Square Error (RMSE): {data['rmse']:.6f}\")\n","            if 'mape' in data:\n","                report.append(f\"  • Mean Absolute Percentage Error (MAPE): {data['mape']:.2f}%\")\n","            if 'r2_score' in data:\n","                report.append(f\"  • R² Score: {data['r2_score']:.4f}\")\n","\n","            # Correlation metrics\n","            report.append(\"Correlation Metrics:\")\n","            if 'pearson_correlation' in data:\n","                report.append(f\"  • Pearson correlation: {data['pearson_correlation']:.4f} \"\n","                            f\"(p-value: {data.get('pearson_p_value', 'N/A'):.2e})\")\n","            if 'spearman_correlation' in data:\n","                report.append(f\"  • Spearman correlation: {data['spearman_correlation']:.4f}\")\n","\n","            # Bias analysis\n","            if 'bias' in data:\n","                report.append(\"Bias Analysis:\")\n","                report.append(f\"  • Absolute bias: {data['bias']:.6f}\")\n","                report.append(f\"  • Relative bias: {data.get('relative_bias_percent', 0):.2f}%\")\n","\n","            # Accuracy within tolerance\n","            report.append(\"Accuracy within tolerance:\")\n","            if 'within_1pct_accuracy' in data:\n","                report.append(f\"  • Within ±1%: {data['within_1pct_accuracy']:.1f}%\")\n","            if 'within_2pct_accuracy' in data:\n","                report.append(f\"  • Within ±2%: {data['within_2pct_accuracy']:.1f}%\")\n","            if 'within_5pct_accuracy' in data:\n","                report.append(f\"  • Within ±5%: {data['within_5pct_accuracy']:.1f}%\")\n","\n","            report.append(\"\")\n","\n","        # Consistency analysis\n","        consistency_data = validation_results.get('consistency_analysis')\n","        if consistency_data:\n","            report.append(\"INTER-METHOD CONSISTENCY ANALYSIS\")\n","            report.append(\"-\" * 40)\n","\n","            mean_cv = consistency_data.get('mean_coefficient_variation', 0)\n","            report.append(f\"Mean coefficient of variation across methods: {mean_cv:.4f}\")\n","\n","            if mean_cv < 0.05:\n","                consistency_rating = \"Excellent\"\n","            elif mean_cv < 0.1:\n","                consistency_rating = \"Good\"\n","            elif mean_cv < 0.2:\n","                consistency_rating = \"Moderate\"\n","            else:\n","                consistency_rating = \"Poor\"\n","\n","            report.append(f\"Method consistency rating: {consistency_rating}\")\n","\n","            correlations = consistency_data.get('method_correlations', {})\n","            if correlations:\n","                report.append(\"\\nPairwise method correlations:\")\n","                for pair, corr_data in correlations.items():\n","                    correlation = corr_data['correlation']\n","                    n_samples = corr_data['n_samples']\n","                    pair_clean = pair.replace('_vs_', ' vs ').replace('_', ' ').title()\n","                    report.append(f\"  • {pair_clean}: r = {correlation:.3f} (n = {n_samples})\")\n","\n","            report.append(\"\")\n","\n","        # Overall recommendations\n","        report.append(\"RECOMMENDATIONS\")\n","        report.append(\"-\" * 40)\n","\n","        best_method = None\n","        best_r2 = -1\n","\n","        for method, data in validation_results.items():\n","            if method != 'consistency_analysis' and 'r2_score' in data:\n","                if data['r2_score'] > best_r2:\n","                    best_r2 = data['r2_score']\n","                    best_method = method\n","\n","        if best_method:\n","            report.append(f\"Best performing method: {best_method.replace('_', ' ').title()} (R² = {best_r2:.4f})\")\n","\n","        # Quality thresholds\n","        report.append(\"\\nQuality Assessment:\")\n","        high_quality_methods = []\n","        for method, data in validation_results.items():\n","            if method != 'consistency_analysis':\n","                r2 = data.get('r2_score', 0)\n","                mae = data.get('mae', float('inf'))\n","                within_2pct = data.get('within_2pct_accuracy', 0)\n","\n","                if r2 > 0.95 and mae < 0.01 and within_2pct > 90:\n","                    high_quality_methods.append(method)\n","\n","        if high_quality_methods:\n","            report.append(f\"High-quality methods (R² > 0.95, MAE < 0.01, >90% within ±2%): \"\n","                         f\"{', '.join([m.replace('_', ' ').title() for m in high_quality_methods])}\")\n","        else:\n","            report.append(\"No methods meet all high-quality criteria. Consider method optimization.\")\n","\n","        report.append(\"\")\n","        report.append(\"=\" * 80)\n","\n","        return \"\\n\".join(report)\n","\n","# Example usage and integration functions\n","def run_complete_validation_pipeline(simulator, simulation_results):\n","    \"\"\"\n","    Run the complete validation pipeline on simulation results\n","\n","    Args:\n","        simulator: BisulfiteSimulator instance\n","        simulation_results: Results from simulation\n","\n","    Returns:\n","        dict: Complete validation results with plots\n","    \"\"\"\n","    print(\"\\n=== Running Complete Validation Pipeline ===\")\n","\n","    # Initialize validation framework\n","    validator = ValidationFramework()\n","    visualizer = VisualizationSuite()\n","\n","    # Extract true efficiencies and analysis results\n","    true_efficiencies = list(simulation_results.keys())\n","\n","    # Mock analysis results structure for demonstration\n","    # In real implementation, this would come from the analyzer\n","    analysis_results = {}\n","\n","    for efficiency in true_efficiencies:\n","        # Simulate some analysis results with realistic noise\n","        noise = np.random.normal(0, 0.005)  # Small amount of noise\n","\n","        analysis_results[efficiency] = {\n","            'non_cpg_efficiency': {'efficiency': efficiency + noise},\n","            'lambda_efficiency': {'efficiency': efficiency + noise * 0.8},\n","            'chh_efficiency': {'efficiency': efficiency + noise * 1.2},\n","            'confidence_intervals': {'mean_efficiency': efficiency + noise * 0.9},\n","            'summary_metrics': {\n","                'consensus_efficiency': efficiency + noise,\n","                'methods_efficiencies': {\n","                    'non_cpg': efficiency + noise,\n","                    'lambda_dna': efficiency + noise * 0.8,\n","                    'chh_context': efficiency + noise * 1.2\n","                }\n","            }\n","        }\n","\n","    # Run validation\n","    validation_results = validator.validate_method_accuracy(analysis_results, true_efficiencies)\n","\n","    # Create visualizations\n","    figures = visualizer.create_accuracy_plots(validation_results)\n","\n","    # Generate report\n","    report = visualizer.create_comprehensive_report(validation_results)\n","\n","    return {\n","        'validation_results': validation_results,\n","        'figures': figures,\n","        'report': report\n","    }\n","\n","if __name__ == \"__main__\":\n","    print(\"Bisulfite Conversion Efficiency Validation and Visualization Module\")\n","    print(\"This module provides comprehensive validation and visualization tools.\")\n","    print(\"Use in conjunction with simulation and analysis modules for complete pipeline.\")"]}]}